{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Load NASBench library and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0  498M    0 1086k    0     0  1086k      0  0:07:49  0:00:01  0:07:48  827k\n",
      "  2  498M    2 11.0M    0     0  5656k      0  0:01:30  0:00:02  0:01:28 4957k\n",
      "  4  498M    4 22.9M    0     0  7824k      0  0:01:05  0:00:03  0:01:02 7151k\n",
      "  6  498M    6 33.6M    0     0  8624k      0  0:00:59  0:00:04  0:00:55 8056k\n",
      "  9  498M    9 45.0M    0     0  9235k      0  0:00:55  0:00:05  0:00:50 9351k\n",
      " 11  498M   11 55.8M    0     0  9528k      0  0:00:53  0:00:06  0:00:47 11.0M\n",
      " 13  498M   13 65.4M    0     0  9570k      0  0:00:53  0:00:07  0:00:46 10.8M\n",
      " 15  498M   15 76.7M    0     0  9828k      0  0:00:51  0:00:08  0:00:43 10.7M\n",
      " 17  498M   17 87.4M    0     0  9949k      0  0:00:51  0:00:09  0:00:42 10.7M\n",
      " 19  498M   19 97.8M    0     0   9.7M      0  0:00:50  0:00:10  0:00:40 10.5M\n",
      " 21  498M   21  109M    0     0   9.9M      0  0:00:50  0:00:11  0:00:39 10.6M\n",
      " 23  498M   23  118M    0     0   9.8M      0  0:00:50  0:00:12  0:00:38 10.6M\n",
      " 26  498M   26  129M    0     0   9.9M      0  0:00:49  0:00:13  0:00:36 10.6M\n",
      " 28  498M   28  141M    0     0  10.0M      0  0:00:49  0:00:14  0:00:35 10.7M\n",
      " 30  498M   30  152M    0     0  10.1M      0  0:00:49  0:00:15  0:00:34 10.9M\n",
      " 32  498M   32  163M    0     0  10.2M      0  0:00:48  0:00:16  0:00:32 10.8M\n",
      " 34  498M   34  173M    0     0  10.1M      0  0:00:48  0:00:17  0:00:31 10.8M\n",
      " 36  498M   36  181M    0     0  10.0M      0  0:00:49  0:00:18  0:00:31 10.3M\n",
      " 38  498M   38  192M    0     0  10.1M      0  0:00:49  0:00:19  0:00:30 10.3M\n",
      " 40  498M   40  204M    0     0  10.2M      0  0:00:48  0:00:20  0:00:28 10.3M\n",
      " 43  498M   43  215M    0     0  10.2M      0  0:00:48  0:00:21  0:00:27 10.3M\n",
      " 45  498M   45  224M    0     0  10.2M      0  0:00:48  0:00:22  0:00:26 10.3M\n",
      " 47  498M   47  235M    0     0  10.2M      0  0:00:48  0:00:23  0:00:25 10.8M\n",
      " 49  498M   49  246M    0     0  10.2M      0  0:00:48  0:00:24  0:00:24 10.6M\n",
      " 51  498M   51  257M    0     0  10.3M      0  0:00:48  0:00:25  0:00:23 10.7M\n",
      " 53  498M   53  269M    0     0  10.3M      0  0:00:48  0:00:26  0:00:22 10.7M\n",
      " 56  498M   56  279M    0     0  10.3M      0  0:00:48  0:00:27  0:00:21 10.9M\n",
      " 58  498M   58  289M    0     0  10.3M      0  0:00:48  0:00:28  0:00:20 10.8M\n",
      " 60  498M   60  300M    0     0  10.3M      0  0:00:48  0:00:29  0:00:19 10.8M\n",
      " 62  498M   62  310M    0     0  10.3M      0  0:00:48  0:00:30  0:00:18 10.4M\n",
      " 64  498M   64  321M    0     0  10.3M      0  0:00:48  0:00:31  0:00:17 10.4M\n",
      " 66  498M   66  332M    0     0  10.3M      0  0:00:47  0:00:32  0:00:15 10.6M\n",
      " 68  498M   68  342M    0     0  10.3M      0  0:00:48  0:00:33  0:00:15 10.4M\n",
      " 70  498M   70  352M    0     0  10.3M      0  0:00:48  0:00:34  0:00:14 10.4M\n",
      " 72  498M   72  363M    0     0  10.3M      0  0:00:48  0:00:35  0:00:13 10.6M\n",
      " 75  498M   75  374M    0     0  10.4M      0  0:00:47  0:00:36  0:00:11 10.6M\n",
      " 76  498M   76  383M    0     0  10.3M      0  0:00:48  0:00:37  0:00:11 10.1M\n",
      " 78  498M   78  389M    0     0  10.2M      0  0:00:48  0:00:38  0:00:10 9728k\n",
      " 80  498M   80  400M    0     0  10.2M      0  0:00:48  0:00:39  0:00:09 9923k\n",
      " 82  498M   82  412M    0     0  10.3M      0  0:00:48  0:00:40  0:00:08 9964k\n",
      " 84  498M   84  423M    0     0  10.3M      0  0:00:48  0:00:41  0:00:07 9964k\n",
      " 87  498M   87  434M    0     0  10.3M      0  0:00:48  0:00:42  0:00:06 10.2M\n",
      " 89  498M   89  445M    0     0  10.3M      0  0:00:48  0:00:43  0:00:05 11.1M\n",
      " 91  498M   91  454M    0     0  10.3M      0  0:00:48  0:00:44  0:00:04 10.8M\n",
      " 93  498M   93  465M    0     0  10.3M      0  0:00:48  0:00:45  0:00:03 10.7M\n",
      " 95  498M   95  476M    0     0  10.3M      0  0:00:48  0:00:46  0:00:02 10.6M\n",
      " 97  498M   97  487M    0     0  10.3M      0  0:00:48  0:00:47  0:00:01 10.6M\n",
      "100  498M  100  498M    0     0  10.3M      0  0:00:48  0:00:48 --:--:-- 10.8M\n",
      "fatal: destination path 'nasbench' already exists and is not an empty directory.\n",
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 186, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 258, in run\n",
      "    isolated_mode=options.isolated_mode,\n",
      "  File \"C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 604, in decide_user_install\n",
      "    if site_packages_writable(root=root_path, isolated=isolated_mode):\n",
      "  File \"C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 549, in site_packages_writable\n",
      "    test_writable_dir(d) for d in set(get_lib_location_guesses(**kwargs))\n",
      "  File \"C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 549, in <genexpr>\n",
      "    test_writable_dir(d) for d in set(get_lib_location_guesses(**kwargs))\n",
      "  File \"C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pip\\_internal\\utils\\filesystem.py\", line 140, in test_writable_dir\n",
      "    return _test_writable_dir_win(path)\n",
      "  File \"C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\pip\\_internal\\utils\\filesystem.py\", line 153, in _test_writable_dir_win\n",
      "    fd = os.open(file, os.O_RDWR | os.O_CREAT | os.O_EXCL)\n",
      "PermissionError: [Errno 13] Permission denied: 'C:\\\\Program Files\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\Lib\\\\site-packages\\\\accesstest_deleteme_fishfingers_custard_gnj91a'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'absl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b97367149a54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Initialize the NASBench object which parses the raw data into memory (this\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# should only be run once as it takes up to a few minutes).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnasbench\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Use nasbench_full.tfrecord for full dataset (run download command above).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\nasbench\\nasbench\\api.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnasbench\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnasbench\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnasbench\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_metrics_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\nasbench\\nasbench\\lib\\config.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mabsl\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mFLAGS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'absl'"
     ]
    }
   ],
   "source": [
    "# This code was written in TF 1.12 but should be supported all the way through\n",
    "# TF 1.15. Untested in TF 2.0+.\n",
    "#%tensorflow_version 1.x\n",
    "\n",
    "# Download the raw data (only 108 epoch data points, for full dataset,\n",
    "# uncomment the second line for nasbench_full.tfrecord).\n",
    "\n",
    "!curl -O https://storage.googleapis.com/nasbench/nasbench_only108.tfrecord\n",
    "# !curl -O https://storage.googleapis.com/nasbench/nasbench_full.tfrecord\n",
    "\n",
    "# Clone and install the code and dependencies.\n",
    "\n",
    "!git clone https://github.com/google-research/nasbench\n",
    "!pip install ./nasbench\n",
    "\n",
    "# Initialize the NASBench object which parses the raw data into memory (this\n",
    "# should only be run once as it takes up to a few minutes).\n",
    "from nasbench import api\n",
    "\n",
    "# Use nasbench_full.tfrecord for full dataset (run download command above).\n",
    "nasbench = api.NASBench('nasbench_only108.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-913112db4472>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#!pip install ./pymap_elites\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpymap_elites\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msetup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\nasbench\\pymap_elites\\setup.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m setup(name='map_elites',\n\u001b[0;32m      3\u001b[0m       \u001b[0mversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'0.1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m       \u001b[0mpackages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_packages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'numpy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sklearn'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m       )\n",
      "\u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\setuptools\\__init__.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(cls, where, exclude, include)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         return list(cls._find_packages_iter(\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[0mconvert_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ez_setup'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'*__pycache__'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             cls._build_filter(*include)))\n",
      "\u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\distutils\\util.py\u001b[0m in \u001b[0;36mconvert_path\u001b[1;34m(pathname)\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"path '%s' cannot end with '/'\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m     \u001b[0mpaths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;34m'.'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[0mpaths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "#to set up map_elites\n",
    "!git clone https://github.com/resibots/pymap_elites\n",
    "!pip install ./pymap_elites\n",
    "\n",
    "from pymap_elites import setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'map_elites'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-6168a11d96fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#import ConfigSpace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmap_elites\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcommon\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#from tabular_benchmarks import FCNetProteinStructureBenchmark, FCNetSliceLocalizationBenchmark,\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'map_elites'"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "#from sklearn.cluster import KMeans\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import os\n",
    "import json\n",
    "from copy import deepcopy\n",
    "#import ConfigSpace\n",
    "from map_elites import common as cm\n",
    "\n",
    "#from tabular_benchmarks import FCNetProteinStructureBenchmark, FCNetSliceLocalizationBenchmark,\\\n",
    "#    FCNetNavalPropulsionBenchmark, FCNetParkinsonsTelemonitoringBenchmark\n",
    "#from tabular_benchmarks import NASCifar10A, NASCifar10B\n",
    "\n",
    "\n",
    "# Useful constants\n",
    "INPUT = 'input'\n",
    "OUTPUT = 'output'\n",
    "CONV3X3 = 'conv3x3-bn-relu'\n",
    "CONV1X1 = 'conv1x1-bn-relu'\n",
    "MAXPOOL3X3 = 'maxpool3x3'\n",
    "NUM_VERTICES = 7\n",
    "MAX_EDGES = 9\n",
    "EDGE_SPOTS = NUM_VERTICES * (NUM_VERTICES - 1) / 2   # Upper triangular matrix\n",
    "OP_SPOTS = NUM_VERTICES - 2   # Input/output vertices are fixed\n",
    "ALLOWED_OPS = [CONV3X3, CONV1X1, MAXPOOL3X3]\n",
    "ALLOWED_EDGES = [0, 1]   # Binary adjacency matrix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Map-Elites implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params = \\\n",
    "    {\n",
    "        # more of this -> higher-quality CVT\n",
    "        \"cvt_samples\": 25000,\n",
    "        # we evaluate in batches to paralleliez\n",
    "        \"batch_size\": 100,\n",
    "        # proportion of niches to be filled before starting\n",
    "        \"random_init\": 0.1,\n",
    "        # batch for random initialization\n",
    "        \"random_init_batch\": 100,\n",
    "        # when to write results (one generation = one batch)\n",
    "        \"dump_period\": 10000,\n",
    "        # do we use several cores?\n",
    "        \"parallel\": True,\n",
    "        # do we cache the result of CVT and reuse?\n",
    "        \"cvt_use_cache\": True,\n",
    "        # min/max of parameters\n",
    "        \"min\": 0,\n",
    "        \"max\": 1,\n",
    "        # only useful if you use the 'iso_dd' variation operator\n",
    "        \"iso_sigma\": 0.01,\n",
    "        \"line_sigma\": 0.2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-690c6c78bd2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[0mn_niches\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m             \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m             \u001b[0mlog_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             variation_operator=cm.variation):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cm' is not defined"
     ]
    }
   ],
   "source": [
    "class Spacies:\n",
    "    def __init__(self, x, desc, fitness, centroid = None):\n",
    "        #x: solution\n",
    "        #desc: description\n",
    "        #fitness: fitness function\n",
    "        self.x = x\n",
    "        self.desc = desc\n",
    "        self.fitness = fitness\n",
    "        self.centroid = centroid\n",
    "        \n",
    "    def polynomial_mutation(x):\n",
    "        y = x.copy()\n",
    "        eta_m = 5.0;\n",
    "        r = np.random.random(size=len(x))\n",
    "        for i in range(0, len(x)):\n",
    "            if r[i] < 0.5:\n",
    "                delta_i = math.pow(2.0 * r[i], 1.0 / (eta_m + 1.0)) - 1.0\n",
    "            else:\n",
    "                delta_i = 1 - math.pow(2.0 * (1.0 - r[i]), 1.0 / (eta_m + 1.0))\n",
    "            y[i] += delta_i\n",
    "        return y\n",
    "\n",
    "    def sbx(x, y, params):\n",
    "        eta = 10.0\n",
    "        xl = params['min']\n",
    "        xu = params['max']\n",
    "        z = x.copy()\n",
    "        r1 = np.random.random(size=len(x))\n",
    "        r2 = np.random.random(size=len(x))\n",
    "    \n",
    "        for i in range(0, len(x)):\n",
    "            if abs(x[i] - y[i]) > 1e-15:\n",
    "                x1 = min(x[i], y[i])\n",
    "                x2 = max(x[i], y[i])\n",
    "    \n",
    "                beta = 1.0 + (2.0 * (x1 - xl) / (x2 - x1))\n",
    "                alpha = 2.0 - beta ** -(eta + 1)\n",
    "                rand = r1[i]\n",
    "                if rand <= 1.0 / alpha:\n",
    "                    beta_q = (rand * alpha) ** (1.0 / (eta + 1))\n",
    "                else:\n",
    "                    beta_q = (1.0 / (2.0 - rand * alpha)) ** (1.0 / (eta + 1))\n",
    "    \n",
    "                c1 = 0.5 * (x1 + x2 - beta_q * (x2 - x1))\n",
    "    \n",
    "                beta = 1.0 + (2.0 * (xu - x2) / (x2 - x1))\n",
    "                alpha = 2.0 - beta ** -(eta + 1)\n",
    "                if rand <= 1.0 / alpha:\n",
    "                    beta_q = (rand * alpha) ** (1.0 / (eta + 1))\n",
    "                else:\n",
    "                    beta_q = (1.0 / (2.0 - rand * alpha)) ** (1.0 / (eta + 1))\n",
    "                c2 = 0.5 * (x1 + x2 + beta_q * (x2 - x1))\n",
    "    \n",
    "                c1 = min(max(c1, xl), xu)\n",
    "                c2 = min(max(c2, xl), xu)\n",
    "    \n",
    "                if r2[i] <= 0.5:\n",
    "                    z[i] = c2\n",
    "                else:\n",
    "                    z[i] = c1\n",
    "        return z\n",
    "    \n",
    "    \n",
    "    def iso_dd(x, y, params):\n",
    "        assert(x.shape == y.shape)\n",
    "        p_max = np.array(params[\"max\"])\n",
    "        p_min = np.array(params[\"min\"])\n",
    "        a = np.random.normal(0, params['iso_sigma'], size=len(x))\n",
    "        b = np.random.normal(0, params['line_sigma'])\n",
    "        norm = np.linalg.norm(x - y)\n",
    "        z = x.copy() + a + b * (x - y)\n",
    "        return np.clip(z, p_min, p_max)\n",
    "    \n",
    "    \n",
    "    def variation(x, z, params):\n",
    "        assert(x.shape == z.shape)\n",
    "        y = sbx(x, z, params)\n",
    "        return y\n",
    "    \n",
    "    def __centroids_filename(k, dim):\n",
    "        return 'centroids_' + str(k) + '_' + str(dim) + '.dat'\n",
    "    \n",
    "    \n",
    "    def __write_centroids(centroids):\n",
    "        k = centroids.shape[0]\n",
    "        dim = centroids.shape[1]\n",
    "        filename = __centroids_filename(k, dim)\n",
    "        with open(filename, 'w') as f:\n",
    "            for p in centroids:\n",
    "                for item in p:\n",
    "                    f.write(str(item) + ' ')\n",
    "                f.write('\\n')\n",
    "\n",
    "    \n",
    "    def cvt(k, dim, samples, cvt_use_cache=True):\n",
    "        # check if we have cached values\n",
    "        fname = __centroids_filename(k, dim)\n",
    "        if cvt_use_cache:\n",
    "            if Path(fname).is_file():\n",
    "                print(\"WARNING: using cached CVT:\", fname)\n",
    "                return np.loadtxt(fname)\n",
    "        # otherwise, compute cvt\n",
    "        print(\"Computing CVT (this can take a while...):\", fname)\n",
    "    \n",
    "        x = np.random.rand(samples, dim)\n",
    "        k_means = KMeans(init='k-means++', n_clusters=k,\n",
    "                         n_init=1, n_jobs=-1, verbose=1)#,algorithm=\"full\")\n",
    "        k_means.fit(x)\n",
    "        __write_centroids(k_means.cluster_centers_)\n",
    "    \n",
    "        return k_means.cluster_centers_\n",
    "    \n",
    "    \n",
    "    def make_hashable(array):\n",
    "        return tuple(map(float, array))\n",
    "    \n",
    "    \n",
    "    def parallel_eval(evaluate_function, to_evaluate, pool, params):\n",
    "        if params['parallel'] == True:\n",
    "            s_list = pool.map(evaluate_function, to_evaluate)\n",
    "        else:\n",
    "            s_list = map(evaluate_function, to_evaluate)\n",
    "        return list(s_list)\n",
    "    \n",
    "    # format: fitness, centroid, desc, genome \\n\n",
    "    # fitness, centroid, desc and x are vectors\n",
    "    def __save_archive(archive, gen):\n",
    "        def write_array(a, f):\n",
    "            for i in a:\n",
    "                f.write(str(i) + ' ')\n",
    "        filename = 'archive_' + str(gen) + '.dat'\n",
    "        with open(filename, 'w') as f:\n",
    "            for k in archive.values():\n",
    "                f.write(str(k.fitness) + ' ')\n",
    "                write_array(k.centroid, f)\n",
    "                write_array(k.desc, f)\n",
    "                write_array(k.x, f)\n",
    "                f.write(\"\\n\")  \n",
    "\n",
    "    # map-elites algorithm (CVT variant)\n",
    "def compute(dim_map, dim_x, f,\n",
    "            n_niches=1000,\n",
    "            max_evals=1e5,\n",
    "            params=cm.default_params,\n",
    "            log_file=None,\n",
    "            variation_operator=cm.variation):\n",
    "    \"\"\"CVT MAP-Elites\n",
    "       Vassiliades V, Chatzilygeroudis K, Mouret JB. Using centroidal voronoi tessellations to scale up the multidimensional archive of phenotypic elites algorithm. IEEE Transactions on Evolutionary Computation. 2017 Aug 3;22(4):623-30.\n",
    "       Format of the logfile: evals archive_size max mean median 5%_percentile, 95%_percentile\n",
    "    \"\"\"\n",
    "    # setup the parallel processing pool\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    pool = multiprocessing.Pool(num_cores)\n",
    "\n",
    "    # create the CVT\n",
    "    c = cm.cvt(n_niches, dim_map,\n",
    "              params['cvt_samples'], params['cvt_use_cache'])\n",
    "    kdt = KDTree(c, leaf_size=30, metric='euclidean')\n",
    "    cm.__write_centroids(c)\n",
    "\n",
    "    archive = {} # init archive (empty)\n",
    "    n_evals = 0 # number of evaluations since the beginning\n",
    "    b_evals = 0 # number evaluation since the last dump\n",
    "\n",
    "    # main loop\n",
    "    while (n_evals < max_evals):\n",
    "        to_evaluate = []\n",
    "        # random initialization\n",
    "        if len(archive) <= params['random_init'] * n_niches:\n",
    "            for i in range(0, params['random_init_batch']):\n",
    "                x = np.random.uniform(low=params['min'], high=params['max'], size=dim_x)\n",
    "                to_evaluate += [(x, f)]\n",
    "        else:  # variation/selection loop\n",
    "            keys = list(archive.keys())\n",
    "            # we select all the parents at the same time because randint is slow\n",
    "            rand1 = np.random.randint(len(keys), size=params['batch_size'])\n",
    "            rand2 = np.random.randint(len(keys), size=params['batch_size'])\n",
    "            for n in range(0, params['batch_size']):\n",
    "                # parent selection\n",
    "                x = archive[keys[rand1[n]]]\n",
    "                y = archive[keys[rand2[n]]]\n",
    "                # copy & add variation\n",
    "                z = variation_operator(x.x, y.x, params)\n",
    "                to_evaluate += [(z, f)]\n",
    "        # evaluation of the fitness for to_evaluate\n",
    "        s_list = cm.parallel_eval(__evaluate, to_evaluate, pool, params)\n",
    "        # natural selection\n",
    "        for s in s_list:\n",
    "            __add_to_archive(s, s.desc, archive, kdt)\n",
    "        # count evals\n",
    "        n_evals += len(to_evaluate)\n",
    "        b_evals += len(to_evaluate)\n",
    "\n",
    "        # write archive\n",
    "        if b_evals >= params['dump_period'] and params['dump_period'] != -1:\n",
    "            print(\"[{}/{}]\".format(n_evals, int(max_evals)), end=\" \", flush=True)\n",
    "            cm.__save_archive(archive, n_evals)\n",
    "            b_evals = 0\n",
    "        # write log\n",
    "        if log_file != None:\n",
    "            fit_list = np.array([x.fitness for x in archive.values()])\n",
    "            log_file.write(\"{} {} {} {} {} {} {}\\n\".format(n_evals, len(archive.keys()),\n",
    "                    fit_list.max(), np.mean(fit_list), np.median(fit_list),\n",
    "                    np.percentile(fit_list, 5), np.percentile(fit_list, 95)))\n",
    "            log_file.flush()\n",
    "    cm.__save_archive(archive, n_evals)\n",
    "    return archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--run_id [RUN_ID]]\n",
      "                             [--benchmark [BENCHMARK]] [--n_iters [N_ITERS]]\n",
      "                             [--output_path [OUTPUT_PATH]]\n",
      "                             [--data_dir [DATA_DIR]] [--pop_size [POP_SIZE]]\n",
      "                             [--sample_size [SAMPLE_SIZE]]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\LG\\AppData\\Roaming\\jupyter\\runtime\\kernel-9a81f7fe-3149-41db-88e7-eec6624d4d8b.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--run_id', default=0, type=int, nargs='?', help='unique number to identify this run')\n",
    "parser.add_argument('--benchmark', default=\"protein_structure\", type=str, nargs='?', help='specifies the benchmark')\n",
    "parser.add_argument('--n_iters', default=100, type=int, nargs='?', help='number of iterations for optimization method')\n",
    "parser.add_argument('--output_path', default=\"./\", type=str, nargs='?',\n",
    "                    help='specifies the path where the results will be saved')\n",
    "parser.add_argument('--data_dir', default=\"./\", type=str, nargs='?', help='specifies the path to the tabular data')\n",
    "parser.add_argument('--pop_size', default=100, type=int, nargs='?', help='population size')\n",
    "parser.add_argument('--sample_size', default=10, type=int, nargs='?', help='sample_size')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.benchmark == \"nas_cifar10a\":\n",
    "    b = NASCifar10A(data_dir=args.data_dir)\n",
    "\n",
    "elif args.benchmark == \"nas_cifar10b\":\n",
    "    b = NASCifar10B(data_dir=args.data_dir)\n",
    "\n",
    "elif args.benchmark == \"protein_structure\":\n",
    "    b = FCNetProteinStructureBenchmark(data_dir=args.data_dir)\n",
    "\n",
    "elif args.benchmark == \"slice_localization\":\n",
    "    b = FCNetSliceLocalizationBenchmark(data_dir=args.data_dir)\n",
    "\n",
    "elif args.benchmark == \"naval_propulsion\":\n",
    "    b = FCNetNavalPropulsionBenchmark(data_dir=args.data_dir)\n",
    "\n",
    "elif args.benchmark == \"parkinsons_telemonitoring\":\n",
    "    b = FCNetParkinsonsTelemonitoringBenchmark(data_dir=args.data_dir)\n",
    "\n",
    "output_path = os.path.join(args.output_path, \"regularized_evolution\")\n",
    "os.makedirs(os.path.join(output_path), exist_ok=True)\n",
    "\n",
    "cs = b.get_configuration_space()\n",
    "\n",
    "history = regularized_evolution(\n",
    "    cycles=args.n_iters, population_size=args.pop_size, sample_size=args.sample_size)\n",
    "\n",
    "if args.benchmark == \"nas_cifar10a\" or args.benchmark == \"nas_cifar10b\":\n",
    "    res = b.get_results(ignore_invalid_configs=True)\n",
    "else:\n",
    "    res = b.get_results()\n",
    "\n",
    "fh = open(os.path.join(output_path, 'run_%d.json' % args.run_id), 'w')\n",
    "json.dump(res, fh)\n",
    "fh.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
